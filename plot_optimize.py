#!/usr/bin/env python3
"""Plot optimizer outputs generated by optimize_prediction_error.py."""
from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from statistics import mean
from typing import Any, Sequence

import matplotlib.pyplot as plt
import numpy as np


@dataclass(frozen=True)
class TrialRecord:
    """Generic representation of optimizer trial outputs."""

    params: dict[str, Any]
    metrics: dict[str, Any]

    @property
    def score(self) -> float:
        return float(self.metrics.get("score", float("inf")))

    @property
    def mean_abs_error(self) -> float:
        return float(self.metrics.get("mean_abs_error", float("inf")))

    @property
    def prediction_failures(self) -> int:
        return int(self.metrics.get("prediction_failures", 0))

    @property
    def avg_eval_bursting_columns(self) -> float:
        return float(self.metrics.get("avg_eval_bursting_columns", 0.0))

    @property
    def valid(self) -> bool:
        value = self.metrics.get("valid", self.metrics.get("passes", True))
        return bool(value)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Render diagnostic plots and/or print summaries for optimizer JSON outputs. "
            "Supports both prediction_error_results*.json (params/metrics) and "
            "hyperparam_search_results.json (config/metrics)."
        )
    )
    parser.add_argument(
        "--results",
        type=Path,
        default=Path("./prediction_error_results_20260110_030858.json"),
        help=(
            "Path to the JSON file emitted by optimize_prediction_error.py or hyperparameter search."
        ),
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("plots"),
        help="Directory where generated figures will be written.",
    )
    parser.add_argument(
        "--dpi",
        type=int,
        default=200,
        help="Dots-per-inch used when saving PNG figures.",
    )
    parser.add_argument(
        "--include-invalid",
        action="store_true",
        help="Include trials marked as invalid in the plots.",
    )
    parser.add_argument(
        "--top-n",
        type=int,
        default=50,
        help="Number of top trials to show in the printed table.",
    )
    return parser.parse_args()


def load_trials(results_path: Path) -> list[TrialRecord]:
    payload = json.loads(results_path.read_text())
    trials: list[TrialRecord] = []
    for entry in payload.get("results", []):
        params = dict(entry.get("params", entry.get("config", {})))
        metrics = dict(entry.get("metrics", {}))
        if params and metrics:
            trials.append(TrialRecord(params=params, metrics=metrics))

    if not trials:
        raise ValueError(f"No trial entries found in {results_path}")
    return trials


def _format_scalar(value: Any) -> str:
    if value is None:
        return "-"
    if isinstance(value, bool):
        return "yes" if value else "no"
    if isinstance(value, int):
        return str(value)
    if isinstance(value, float):
        if not np.isfinite(value):
            return "inf"
        return f"{value:.6g}"
    return str(value)


def _format_json_block(payload: dict[str, Any]) -> str:
    return json.dumps(payload, indent=2, sort_keys=True)


def _select_metric_keys(records: Sequence[TrialRecord]) -> list[str]:
    # Prefer commonly used metrics, but only show keys that appear.
    preferred = [
        "valid",
        "passes",
        "mean_abs_error",
        "prediction_failures",
        "avg_eval_bursting_columns",
        "evaluation_burst_sum",
        "final_burst",
        "max_initial_bursts",
    ]
    available: set[str] = set()
    for rec in records:
        available.update(rec.metrics.keys())
    available.discard("score")
    selected = [key for key in preferred if key in available]

    # Keep the printed table compact; the full best-trial metrics are printed above.
    remaining = [key for key in sorted(available) if key not in selected]
    while len(selected) < 6 and remaining:
        selected.append(remaining.pop(0))
    return selected


def _load_config(results_path: Path) -> dict[str, Any]:
    payload = json.loads(results_path.read_text())
    config = payload.get("config", {})
    if isinstance(config, dict):
        return dict(config)
    return {}


def _total_cells_from_config(config: dict[str, Any]) -> int | None:
    try:
        num_columns = int(config.get("num_columns"))
        cells_per_column = int(config.get("cells_per_column"))
    except (TypeError, ValueError):
        return None
    if num_columns <= 0 or cells_per_column <= 0:
        return None
    return num_columns * cells_per_column


def _segments_per_cell(record: TrialRecord, total_cells: int | None) -> float | None:
    if total_cells is None:
        return None
    if "total_segments" not in record.metrics:
        return None
    try:
        total_segments = float(record.metrics["total_segments"])
    except (TypeError, ValueError):
        return None
    if total_cells <= 0:
        return None
    return total_segments / total_cells


def _synapses_per_segment(record: TrialRecord) -> float | None:
    if "total_segments" not in record.metrics or "total_synapses" not in record.metrics:
        return None
    try:
        total_segments = float(record.metrics["total_segments"])
        total_synapses = float(record.metrics["total_synapses"])
    except (TypeError, ValueError):
        return None
    if total_segments <= 0:
        return 0.0
    return total_synapses / total_segments


def pretty_print_trials(records: Sequence[TrialRecord], results_path: Path, top_n: int) -> None:
    sorted_records = sorted(records, key=lambda rec: rec.mean_abs_error)
    metric_keys = _select_metric_keys(sorted_records)

    config = _load_config(results_path)
    total_cells = _total_cells_from_config(config)
    include_segments_per_cell = total_cells is not None and any(
        "total_segments" in rec.metrics for rec in sorted_records
    )
    include_synapses_per_segment = any(
        "total_segments" in rec.metrics and "total_synapses" in rec.metrics
        for rec in sorted_records
    )

    invalid_count = sum(1 for rec in sorted_records if not rec.valid)
    scores = [rec.score for rec in sorted_records if np.isfinite(rec.score)]
    best = sorted_records[0]

    print(f"Results: {results_path}")
    print(f"Trials: {len(sorted_records)} (invalid: {invalid_count})")
    if scores:
        print(
            "Score: "
            f"best={min(scores):.6g}  mean={mean(scores):.6g}  worst={max(scores):.6g}"
        )
    print()
    print("Best trial")
    print("Metrics:")
    print(_format_json_block(best.metrics))
    print("Config:")
    print(_format_json_block(best.params))
    print()

    show_n = max(1, min(top_n, len(sorted_records)))
    headers = ["rank", "score"]
    if include_segments_per_cell:
        headers.append("segments_per_cell")
    if include_synapses_per_segment:
        headers.append("synapses_per_segment")
    headers.extend([*metric_keys, "key_params"])
    rows: list[list[str]] = []
    for idx, rec in enumerate(sorted_records[:show_n], start=1):
        row: list[str] = [str(idx), _format_scalar(rec.score)]
        if include_segments_per_cell:
            row.append(_format_scalar(_segments_per_cell(rec, total_cells)))
        if include_synapses_per_segment:
            row.append(_format_scalar(_synapses_per_segment(rec)))
        for key in metric_keys:
            row.append(_format_scalar(rec.metrics.get(key)))
        # Keep this compact: show only params that vary across trials.
        row.append(", ".join(f"{k}={_format_scalar(rec.params.get(k))}" for k in varying_param_keys(sorted_records)[:3]))
        rows.append(row)

    widths = [len(h) for h in headers]
    for row in rows:
        for col_idx, value in enumerate(row):
            widths[col_idx] = max(widths[col_idx], len(value))

    def emit(row: list[str]) -> None:
        print("  ".join(value.ljust(widths[i]) for i, value in enumerate(row)))

    print(f"Top {show_n} trials")
    emit(headers)
    emit(["-" * w for w in widths])
    for row in rows:
        emit(row)


def numeric_param_values(records: Sequence[TrialRecord], key: str) -> list[float]:
    values: list[float] = []
    for rec in records:
        if key not in rec.params:
            continue
        value = rec.params[key]
        if isinstance(value, (int, float)):
            values.append(float(value))
    return values


def varying_param_keys(records: Sequence[TrialRecord]) -> list[str]:
    keys: set[str] = set()
    for rec in records:
        keys.update(rec.params.keys())
    varying: list[str] = []
    for key in sorted(keys):
        values = numeric_param_values(records, key)
        unique = {value for value in values}
        if len(unique) > 1:
            varying.append(key)
    return varying


def title_from_key(key: str) -> str:
    return key.replace("_", " ")


def param_as_float(record: TrialRecord, key: str) -> float | None:
    if key not in record.params:
        return None
    value = record.params.get(key)
    if isinstance(value, (int, float)):
        return float(value)
    return None


def plot_error_vs_score(records: Sequence[TrialRecord], output_dir: Path, dpi: int) -> None:
    scores = np.array([rec.score for rec in records])
    errors = np.array([rec.mean_abs_error for rec in records])
    bursts = np.array([rec.avg_eval_bursting_columns for rec in records])
    failures = np.array([max(rec.prediction_failures, 1) for rec in records])

    fig, ax = plt.subplots(figsize=(7, 5))
    scatter = ax.scatter(
        scores,
        errors,
        c=bursts,
        cmap="viridis",
        s=40 * np.sqrt(failures),
        alpha=0.85,
        edgecolors="k",
        linewidths=0.3,
    )
    best = min(records, key=lambda rec: rec.score)
    ax.scatter(
        [best.score],
        [best.mean_abs_error],
        marker="*",
        s=220,
        c="#ff6b6b",
        edgecolors="k",
        linewidths=0.6,
        label="Best score",
        zorder=5,
    )
    ax.set_xlabel("Composite score (lower is better)")
    ax.set_ylabel("Mean absolute prediction error")
    ax.set_title("Error vs score (color = eval bursts, size = failures)")
    ax.grid(True, linestyle=":", alpha=0.4)
    ax.legend(loc="upper right")
    fig.colorbar(scatter, ax=ax, label="Avg eval bursting columns")
    fig.tight_layout()
    fig.savefig(output_dir / "score_vs_mean_error.png", dpi=dpi, bbox_inches="tight")
    plt.close(fig)


def plot_param_sensitivity(records: Sequence[TrialRecord], output_dir: Path, dpi: int) -> None:
    varying = varying_param_keys(records)
    if not varying:
        return

    y_values = np.array([rec.mean_abs_error for rec in records])
    panels_per_fig = 4

    for figure_idx in range(0, len(varying), panels_per_fig):
        subset = varying[figure_idx : figure_idx + panels_per_fig]
        fig, axes = plt.subplots(2, 2, figsize=(10, 7), sharey=True)
        axes_flat = list(axes.flat)

        for ax in axes_flat[len(subset) :]:
            ax.axis("off")

        for ax, key in zip(axes_flat, subset):
            x_values = np.array(numeric_param_values(records, key), dtype=float)
            if len(x_values) != len(records):
                x_values = np.array([float(rec.params.get(key, np.nan)) for rec in records], dtype=float)

            ax.scatter(x_values, y_values, alpha=0.3, color="#4c72b0", label="Trials")
            unique_levels = sorted({float(v) for v in x_values if not np.isnan(v)})
            aggregated: list[tuple[float, float]] = []
            for level in unique_levels:
                mask = x_values == level
                if not np.any(mask):
                    continue
                aggregated.append((level, float(np.mean(y_values[mask]))))
            if aggregated:
                ax.plot(
                    [level for level, _ in aggregated],
                    [avg for _, avg in aggregated],
                    color="#dd8452",
                    marker="o",
                    label="Mean error",
                )
            ax.set_xlabel(title_from_key(key))
            ax.grid(True, linestyle=":", alpha=0.4)

        axes[0][0].set_ylabel("Mean absolute prediction error")
        axes[1][0].set_ylabel("Mean absolute prediction error")
        axes[0][0].legend()
        fig.suptitle("Parameter sensitivity")
        fig.tight_layout(rect=(0, 0, 1, 0.97))
        suffix = 1 + figure_idx // panels_per_fig
        filename = "parameter_sensitivity.png" if len(varying) <= panels_per_fig else f"parameter_sensitivity_{suffix}.png"
        fig.savefig(output_dir / filename, dpi=dpi, bbox_inches="tight")
        plt.close(fig)


def plot_activation_learning_heatmap(records: Sequence[TrialRecord], output_dir: Path, dpi: int) -> None:
    activation_key = "activation_threshold_pct"
    learning_key = "learning_threshold_pct"
    activation_levels = sorted(
        {
            value
            for rec in records
            if (value := param_as_float(rec, activation_key)) is not None
        }
    )
    learning_levels = sorted(
        {value for rec in records if (value := param_as_float(rec, learning_key)) is not None}
    )
    if len(activation_levels) < 2 or len(learning_levels) < 2:
        return

    heatmap = np.empty((len(activation_levels), len(learning_levels)))
    for i, activation in enumerate(activation_levels):
        for j, learning in enumerate(learning_levels):
            matching = [
                rec
                for rec in records
                if param_as_float(rec, activation_key) == activation
                and param_as_float(rec, learning_key) == learning
            ]
            if not matching:
                heatmap[i, j] = np.nan
                continue
            heatmap[i, j] = float(np.mean([rec.mean_abs_error for rec in matching]))

    fig, ax = plt.subplots(figsize=(7, 5))
    mesh = ax.imshow(
        heatmap,
        interpolation="nearest",
        origin="lower",
        cmap="plasma",
        aspect="auto",
    )
    fig.colorbar(mesh, ax=ax, label="Mean absolute prediction error")
    ax.set_xticks(range(len(learning_levels)))
    ax.set_xticklabels([f"{lvl:.3f}" for lvl in learning_levels], rotation=45, ha="right")
    ax.set_yticks(range(len(activation_levels)))
    ax.set_yticklabels([f"{lvl:.3f}" for lvl in activation_levels])
    ax.set_xlabel("Learning threshold pct")
    ax.set_ylabel("Activation threshold pct")
    ax.set_title("Activation vs learning threshold (error heatmap)")
    fig.tight_layout()
    fig.savefig(output_dir / "activation_learning_heatmap.png", dpi=dpi, bbox_inches="tight")
    plt.close(fig)


def main() -> None:
    args = parse_args()
    records = load_trials(args.results)
    pretty_print_trials(records, args.results, args.top_n)

    plot_records = records
    if not args.include_invalid:
        plot_records = [rec for rec in plot_records if rec.valid]
    if not plot_records:
        raise ValueError("No trials available to plot after filtering.")

    has_finite_errors = any(np.isfinite(rec.mean_abs_error) for rec in plot_records)
    if not has_finite_errors:
        print("Skipping plots: mean_abs_error not present in results.")
        return

    args.output_dir.mkdir(parents=True, exist_ok=True)
    plot_error_vs_score(plot_records, args.output_dir, args.dpi)
    plot_param_sensitivity(plot_records, args.output_dir, args.dpi)
    plot_activation_learning_heatmap(plot_records, args.output_dir, args.dpi)
    print(f"Saved plots to {args.output_dir.resolve()}")


if __name__ == "__main__":
    main()
