#!/usr/bin/env python3
"""Plot optimizer outputs generated by optimize_prediction_error.py."""
from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Sequence

import matplotlib.pyplot as plt
import numpy as np


@dataclass(frozen=True)
class TrialRecord:
    """Flattened representation of optimizer trial metrics."""

    growth_strength: float
    max_synapse_pct: float
    activation_threshold_pct: float
    learning_threshold_pct: float
    mean_abs_error: float
    max_abs_error: float
    prediction_failures: int
    avg_eval_bursting_columns: float
    train_max_initial_burst: int
    train_final_burst: int
    score: float


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Render diagnostic plots for prediction_error_results.json outputs."
    )
    parser.add_argument(
        "--results",
        type=Path,
        default=Path("./prediction_error_results_20260107_005418.json"),
        help="Path to the JSON file emitted by optimize_prediction_error.py.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("plots"),
        help="Directory where generated figures will be written.",
    )
    parser.add_argument(
        "--dpi",
        type=int,
        default=200,
        help="Dots-per-inch used when saving PNG figures.",
    )
    return parser.parse_args()


def load_trials(results_path: Path) -> list[TrialRecord]:
    payload = json.loads(results_path.read_text())
    trials: list[TrialRecord] = []
    for entry in payload.get("results", []):
        params = entry["params"]
        metrics = entry["metrics"]
        trials.append(
            TrialRecord(
                growth_strength=params["growth_strength"],
                max_synapse_pct=params["max_synapse_pct"],
                activation_threshold_pct=params["activation_threshold_pct"],
                learning_threshold_pct=params["learning_threshold_pct"],
                mean_abs_error=metrics["mean_abs_error"],
                max_abs_error=metrics["max_abs_error"],
                prediction_failures=metrics["prediction_failures"],
                avg_eval_bursting_columns=metrics["avg_eval_bursting_columns"],
                train_max_initial_burst=metrics["train_max_initial_burst"],
                train_final_burst=metrics["train_final_burst"],
                score=metrics["score"],
            )
        )
    if not trials:
        raise ValueError(f"No trial entries found in {results_path}")
    return trials


def plot_error_vs_score(records: Sequence[TrialRecord], output_dir: Path, dpi: int) -> None:
    scores = np.array([rec.score for rec in records])
    errors = np.array([rec.mean_abs_error for rec in records])
    bursts = np.array([rec.avg_eval_bursting_columns for rec in records])
    failures = np.array([max(rec.prediction_failures, 1) for rec in records])

    fig, ax = plt.subplots(figsize=(7, 5))
    scatter = ax.scatter(
        scores,
        errors,
        c=bursts,
        cmap="viridis",
        s=40 * np.sqrt(failures),
        alpha=0.85,
        edgecolors="k",
        linewidths=0.3,
    )
    best = min(records, key=lambda rec: rec.score)
    ax.scatter(
        [best.score],
        [best.mean_abs_error],
        marker="*",
        s=220,
        c="#ff6b6b",
        edgecolors="k",
        linewidths=0.6,
        label="Best score",
        zorder=5,
    )
    ax.set_xlabel("Composite score (lower is better)")
    ax.set_ylabel("Mean absolute prediction error")
    ax.set_title("Error vs score (color = eval bursts, size = failures)")
    ax.grid(True, linestyle=":", alpha=0.4)
    ax.legend(loc="upper right")
    fig.colorbar(scatter, ax=ax, label="Avg eval bursting columns")
    fig.tight_layout()
    fig.savefig(output_dir / "score_vs_mean_error.png", dpi=dpi, bbox_inches="tight")
    plt.close(fig)


def plot_param_sensitivity(records: Sequence[TrialRecord], output_dir: Path, dpi: int) -> None:
    fig, axes = plt.subplots(2, 2, figsize=(10, 7), sharey=True)
    param_specs = [
        ("growth_strength", "Growth strength"),
        ("max_synapse_pct", "Max synapse %"),
        ("activation_threshold_pct", "Activation threshold %"),
        ("learning_threshold_pct", "Learning threshold %"),
    ]

    y_values = np.array([rec.mean_abs_error for rec in records])

    for ax, (attr, label) in zip(axes.flat, param_specs):
        x_values = np.array([getattr(rec, attr) for rec in records])
        ax.scatter(x_values, y_values, alpha=0.3, color="#4c72b0", label="Trials")
        unique_levels = sorted(set(x_values))
        aggregated = []
        for level in unique_levels:
            mask = x_values == level
            aggregated.append((level, float(np.mean(y_values[mask]))))
        ax.plot(
            [level for level, _ in aggregated],
            [avg for _, avg in aggregated],
            color="#dd8452",
            marker="o",
            label="Mean error",
        )
        ax.set_xlabel(label)
        ax.grid(True, linestyle=":", alpha=0.4)

    axes[0][0].set_ylabel("Mean absolute prediction error")
    axes[1][0].set_ylabel("Mean absolute prediction error")
    axes[0][0].legend()
    fig.suptitle("Parameter sensitivity")
    fig.tight_layout(rect=(0, 0, 1, 0.97))
    fig.savefig(output_dir / "parameter_sensitivity.png", dpi=dpi, bbox_inches="tight")
    plt.close(fig)


def plot_activation_learning_heatmap(records: Sequence[TrialRecord], output_dir: Path, dpi: int) -> None:
    activation_levels = sorted({rec.activation_threshold_pct for rec in records})
    learning_levels = sorted({rec.learning_threshold_pct for rec in records})
    if len(activation_levels) < 2 or len(learning_levels) < 2:
        return

    lookup = {
        (rec.activation_threshold_pct, rec.learning_threshold_pct): rec
        for rec in records
    }
    heatmap = np.empty((len(activation_levels), len(learning_levels)))
    for i, activation in enumerate(activation_levels):
        for j, learning in enumerate(learning_levels):
            record = lookup.get((activation, learning))
            if record is None:
                heatmap[i, j] = np.nan
            else:
                heatmap[i, j] = record.mean_abs_error

    fig, ax = plt.subplots(figsize=(7, 5))
    mesh = ax.imshow(
        heatmap,
        interpolation="nearest",
        origin="lower",
        cmap="plasma",
        aspect="auto",
    )
    fig.colorbar(mesh, ax=ax, label="Mean absolute prediction error")
    ax.set_xticks(range(len(learning_levels)))
    ax.set_xticklabels([f"{lvl:.3f}" for lvl in learning_levels], rotation=45, ha="right")
    ax.set_yticks(range(len(activation_levels)))
    ax.set_yticklabels([f"{lvl:.3f}" for lvl in activation_levels])
    ax.set_xlabel("Learning threshold pct")
    ax.set_ylabel("Activation threshold pct")
    ax.set_title("Activation vs learning threshold (error heatmap)")
    fig.tight_layout()
    fig.savefig(output_dir / "activation_learning_heatmap.png", dpi=dpi, bbox_inches="tight")
    plt.close(fig)


def main() -> None:
    args = parse_args()
    records = load_trials(args.results)
    args.output_dir.mkdir(parents=True, exist_ok=True)

    plot_error_vs_score(records, args.output_dir, args.dpi)
    plot_param_sensitivity(records, args.output_dir, args.dpi)
    plot_activation_learning_heatmap(records, args.output_dir, args.dpi)
    print(f"Saved plots to {args.output_dir.resolve()}")


if __name__ == "__main__":
    main()
